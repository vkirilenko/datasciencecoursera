---
title: "Practical machine learning: Project"
author: "V. Kirilenko"
date: "Saturday, May 23, 2015"
output: html_document
geometry: margin = 1 cm
---
## Summary  

The purpose of this study was to predict the class of the power lift execution, other words to predict "how well" an activity was performed by the participants. Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Dataset and information can be download from here: [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har). According to the results of machine learning and analysis of the constructed models was achieved overall accuracy of prediction in 98,3% (1 error from 60 predictions).

```{r "setoptions", echo = F, message=F, warning=F}
library(knitr)
library(ggplot2)
library(randomForest)
library(caret)
options(stringsAsFactors = FALSE)
opts_chunk$set(echo = T, cache = F, eval = T)
time = format(Sys.time(), "%X %d/%m/%Y")
```
### _Data Processing_  
First, we need a data for work.  
```{r "Download a data", eval = F}
url1 = "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2 = "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
temp = tempfile()
download.file(url1, temp)
train_tmp = read.csv(temp,na.strings=c("NA",""), header=TRUE, stringsAsFactors = FALSE)
download.file(url2, temp)
test_tmp = read.csv(temp, na.strings=c("NA",""), header=TRUE, stringsAsFactors = FALSE)
unlink(temp)
```
<br>
In our dataset we have a lot of column with NA values or with time values. All of them useless for our study.  
So we must remove them from training and testing datasets.  
```{r "Dividing and preprocessing data", eval = F}
set.seed(3422)
test = test_tmp[,unlist(lapply(1:ncol(test_tmp),function(x) (sum(is.na(test_tmp[,x]))/nrow(test_tmp))<0.9))]
train = train_tmp[,unlist(lapply(1:ncol(train_tmp),function(x) (sum(is.na(train_tmp[,x]))/nrow(train_tmp))<0.9))]
train$classe <- as.factor(train$classe)
train[,6]=NULL; train[,5]=NULL; train[,4]=NULL; train[,3]=NULL; train[,1]=NULL
trcv = trainControl(method="cv", number=10)
test[,6]=NULL; test[,5]=NULL; test[,4]=NULL; test[,3]=NULL; test[,1]=NULL
parts = createDataPartition(y=train$classe, p=.70, list=F)
worktrain = train[parts,]
valid = train[-parts,]
```
  
### _Model training_   
After cleaning data we can build our model.  
We can choose 3 models for independent control of the results.   
    
**First:** _Regularized Discriminant Analysis_.  
This is more robust and simple version of linear discriminant analysis and it is most effective in case of model with many predictors(as in our case).  
This model is made with 10 k-fold cross-validation.  

**Second:** _Stochastic Gradient Boosting_.   
GBM method will build binary trees, i.e., partition the data into two samples at each split node. Its strong side is balance between approximation accuracy and execution speed. In our study it was the fastest model (approx. ~20 min.).
This model is made with 10 k-fold cross-validation.  

**Third:** _Random Forest_.  
This model is one of the most popular machine learning methods thanks to their good accuracy, robustness and ease of use.
Random forest model is very effective in work with variables with non-obvious relationships, but it can be biased towards variables with many categories.  
This model is made without cross-validation, because **random** in the name of the method points to a random sample of predictors to build the tree.  
Therefore, cross-validation does not make any sense. Also this is needed to compare results of the models with cross-validation and without it.  
This model was built with out-of-bag control. The out-of-bag error is internal error estimate of a random forest as it is being constructed.  
```{r "Demo code for training models", eval = F}
train_rda_cv = train(classe ~ ., method="rda", trControl = trcv, tuneLength = 4, data=train)
train_gbm_cv = train(classe ~ ., method="gbm", trControl = trcv, distribution="multinomial", data=train)
train_rf_oob_w = train(classe ~., data=worktrain, trControl = trrf, method = "rf")
rda_test = predict(train_rda_cv, newdata = test)
gbm_test = predict(train_gbm_cv, newdata = test)
rf_test = predict(train_rf_oob_w, newdata = test)
```
  
### _Evaluation of the results_   
Now we can check accuracy and errors of our models, also we can evaluate importance of the predictors.  
```{r "Load the final models", echo = F, eval = T}
train_rda_cv = readRDS("train_rda_cv001.rds")
train_gbm_cv = readRDS("train_gbm_cv001.rds")
train_rf_oob_w = readRDS("train_rf_oob002.rds")
valid = readRDS("valid.RDS")
```

```{r "Summary and plots", eval = T, message = F, warning=F}
variance = varImp(train_gbm_cv, scale = FALSE)
plot(variance, top = 20, main = "Name of the predictor")
train_rf_oob_w$finalModel
resamps = resamples(list(GBM = train_gbm_cv, RDA = train_rda_cv))
summary(resamps)
difValues = diff(resamps)
summary(difValues)
pred = predict(train_rf_oob_w, valid)
confusionMatrix(pred,valid$classe)
```
As we can see, our models have accuracy in range 0.92 - 0.99.  
Usually this means the overfitting of the model. But in our case it is a feature of the test data set.  
In the end we got the predictions for 20 test samples and all of these predictons were identical, except first case.  
So, our independent training of the models help to us and we can get 2 identical predictions and use them.  
In this scenario we have 20 correct predictions for 20 cases.  
  
Also estimating the out-of-sample error tells us that our models are not overfitting, because in-sample error(**`r 1-0.994`**) is very close to OOS error(**`r 1-0.9981`**).  


**Information:`r R.version`**   
**Last edition: `r time`**